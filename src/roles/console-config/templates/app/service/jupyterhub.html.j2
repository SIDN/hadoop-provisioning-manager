<h1 class="display-6">Jupyterhub</h1>
<p>
<a href="https://jupyter.org/hub">Jupyterhub</a> A multi-user version of the notebook designed for companies, classrooms and research labs
</p>
<p>
You are automatically logged-in if an active Kerberos session is detected. Kerberos principal admin@{{ kerberos_realm }} is the default Hue administrator.
Impyla and Pyspark are both available to use for querying data from Hadoop.
</p>
<table class="table table-hover">
  <tr>
    <th>Service</th>
    <th>Host</th>
  </tr>
    {% for host in groups['jupyterhub'] %}
    <tr>
     <td>Jupyterhub</td>
      <td><a href="https://{{ host }}:{{ jupyterhub_port }}" target="_blank">{{ host }}</a></td>
    </tr>
    {% endfor %}
</table>
<br>

<h4>Authentication</h4>
<p>
Impyla and Pyspark both requires an active Kerberos session, upload your keytab to 
your notebook and then use kinit to create a Kerberos session by adding the following at the top
of your notebook.
</p>
<pre><code>
# Create Kerberos session
!kinit user@{{ kerberos_realm }} -kt /path/to/my.keytab
</code></pre>

<h4>Impyla</h4>
<p>
Create an Impyl to the Impala loadbalancer (proxy), make sure to also enable SSL.
</p>
<pre><code>
# run Impyla query
from impala.dbapi import connect
from impala.util import as_pandas

conn = connect(host='{{ groups['impala_ha_proxy'][0] }}', auth_mechanism='GSSAPI', use_ssl=True)
cursor = conn.cursor()

cursor.execute('SELECT ... FROM ... LIMIT ...')
df = as_pandas(cursor)
</code></pre>


<h4>Pyspark</h4>
<p>
Create a Pyspark driver where the Spark Driver (client) is executed on the Jupyterhub server and the
Spark executors are running on the cluster using Yarn containers.
</p>
<pre><code>
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = (SparkSession.builder
    .master('yarn') 
    .config('spark.app.name', 'my notebook')
    .config('spark.submit.deployMode', 'client')
    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
    .config('spark.sql.autoBroadcastJoinThreshold', '100MB')
    .config('spark.sql.parquet.int96AsTimestamp', 'true')
    .config('spark.sql.optimizer.expression.nestedPruning.enabled', 'true')
    .config('spark.sql.parquet.binaryAsString', 'true')
    .config('spark.sql.files.ignoreCorruptFiles', 'true')
    .config('spark.sql.session.timeZone', 'UTC')
    .config('spark.executor.instances', 11)
    .config('spark.executor.cores', 5)
    .getOrCreate())

df = (spark
        .read
        .parquet('/user/path/to/parquet/data')
        .limit(100)
      )
      
data = df.toPandas()
</code></pre>


<h4>Pyhton libraries</h4>
<p>
JupyterLab notbooks have the following Pyhton packages installed by default.

</p>

{{ jupyterhub_default_packages }}