<h1 class="display-6">Jupyterhub</h1>
<p>
<a href="https://jupyter.org/hub">Jupyterhub</a> A multi-user version of <a href="https://jupyter.org/">Jupyterlab</a> notebooks. Impyla and Pyspark are available to analyse the data available on Hadoop.
</p>
<table class="table table-hover">
  <tr>
    <th>Service</th>
    <th>Host</th>
  </tr>
    {% for host in groups['jupyterhub'] %}
    <tr>
     <td>Jupyterhub</td>
      <td><a href="https://{{ host }}:{{ jupyterhub_port }}" target="_blank">{{ host }}</a></td>
    </tr>
    {% endfor %}
</table>
<br>

<h4>Deploy</h4>
<p>
Building and deploying Jupyterhub may take some time, therefore the  <code>deploy.sh all</code> command does not deploy Jupyterhub, use <code>deploy.sh jupyterhub</code>
</p>

<h4>Authentication</h4>
<p>
You are automatically logged-in if an active Kerberos session is detected. Kerberos principal admin@{{ kerberos_realm }} is the default Hue administrator.<br>
Impyla and Pyspark both require an active Kerberos session, upload your keytab and use kinit to create a Kerberos session by adding the following to the top
of your notebook.
</p>
<pre><code>
# Create Kerberos session
!kinit user@{{ kerberos_realm }} -kt /path/to/my.keytab
</code></pre>

<h4>Impyla</h4>
<p>
Create an Impyl to the Impala loadbalancer (proxy), make sure to also enable SSL.
</p>
<pre><code>
# run Impyla query
from impala.dbapi import connect
from impala.util import as_pandas

conn = connect(host='{{ groups['impala_ha_proxy'][0] }}', auth_mechanism='GSSAPI', use_ssl=True)
cursor = conn.cursor()

cursor.execute('SELECT ... FROM ... LIMIT ...')
df = as_pandas(cursor)
</code></pre>

<h4>Pyspark</h4>
<p>
Create a Pyspark driver where the Spark Driver (client) is executed on the Jupyterhub server and the
Spark executors are running on the cluster using Yarn containers.
</p>
<pre><code>
from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = (SparkSession.builder
    .master('yarn') 
    .config('spark.app.name', 'my notebook')
    .config('spark.submit.deployMode', 'client')
    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
    .config('spark.sql.autoBroadcastJoinThreshold', '100MB')
    .config('spark.sql.parquet.int96AsTimestamp', 'true')
    .config('spark.sql.optimizer.expression.nestedPruning.enabled', 'true')
    .config('spark.sql.parquet.binaryAsString', 'true')
    .config('spark.sql.files.ignoreCorruptFiles', 'true')
    .config('spark.sql.session.timeZone', 'UTC')
    .config('spark.executor.instances', 11)
    .config('spark.executor.cores', 5)
    .getOrCreate())

df = (spark
        .read
        .parquet('/user/path/to/parquet/data')
        .limit(100)
      )
      
data = df.toPandas()
</code></pre>


<h4>Python package</h4>
<p>
JupyterLab has the following Python packages installed by default.

</p>

<ul>
{% for p in jupyterhub_default_packages %}
   {% if not p.startswith('#') %}
     <li>{{ p }}</li>
    {% endif %}  
{% endfor %}
</ul>

<p>
Installing additional package can be done by adding these to the configuration file <code>config/python/jupyterhub-user-requirements.txt</code> in your configuration set.
After updating the package list you must redeploy (rebuild the Docker image) Jupyterhub.

</p>

<ul>
{% for p in jupyterhub_user_packages %}
   {% if not p.startswith('#') %}
     <li>{{ p }}</li>
    {% endif %}  
{% endfor %}
</ul>

