# OS user to run spark process
spark_os_user: spark
# Group used for spark OS user
spark_os_group: hadoop

# location of config files
spark_etc_dir: "{{ config_root }}/spark"

# Location of spark logging
spark_log_dir: /var/log/spark

# Spark history server logging directory on HDFS
# The spark jobs themselves must be configured to log events,
spark_history_fs_logdirectory: /user/spark/history

spark_heap:
  history_server: 1g

spark_ports:
  history_ui_port: 18480

# add additional default spark start options
# file: spark-default.conf 
spark_default_options:
  # options for dynamicAllocation per yarn application
  spark.dynamicAllocation.initialExecutors: 1
  spark.dynamicAllocation.minExecutors: 1
  spark.dynamicAllocation.maxExecutors: 2 # depends on number of nodes in cluster
  spark.eventLog.enabled: true
  spark.driver.log.persistToDfs.enabled: true
  spark.history.store.maxDiskUsage: 1G
  spark.history.fs.cleaner.maxAge: 3d
  # Keep spark.history.fs.update.interval high to prevent Solr index filling up fast with audit events
  spark.history.fs.update.interval: 300s
  spark.sql.files.ignoreCorruptFiles: true #ignore any corrupt input files

# Yarn options for thrift server.
# see: start-thriftserver.sh --help
spark_thrift_server_yarn:
  port: 10000
  # name of the YARN queue used by the Spark Thift server
  queue: 'spark-thrift-server' 
  # memory for spark driver of thrift_server
  driver_memory: 2g 
  ui_port: 4049
  executors: 1
  #  The consensus in most Spark tuning guides is that 5 cores per executor is the optimum number
  executor_cores: 5 
  executor_memory: 2g 

# Hive options for used by Spark
# file:  hive-site.xml
spark_hive_options:
  hive.server2.thrift.port: "{{ spark_thrift_server_yarn.port }}"
  
# name of the default python virtual env used by spark
# this venv will be created here: {{ python_venv_dir }}/{{ spark_default_python_venv }}
spark_default_python_venv: 'pyspark-default-venv'

# URL where to download Apache Spark
spark_download_url: "https://dlcdn.apache.org/spark/spark-{{ version.spark }}/spark-{{ version.spark }}-bin-hadoop{{ version.spark_hadoop }}.tgz"
#spark_download_url: "https://dlcdn.apache.org/spark/spark-{{ version.spark }}/spark-{{ version.spark }}-bin-hadoop3.2-{{ version.spark_scala }}.tgz"

spark_additional_libs_download_url:
  - name: 'apache-iceberg'
    url: "https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-{{ version.spark|reverse|replace('.','_',1)|reverse }}.{{ version.spark_scala }}/{{ version.iceberg }}/iceberg-spark-runtime-{{ version.spark|reverse|replace('.','_',1)|reverse }}.{{ version.spark_scala }}-{{ version.iceberg }}.jar"
  - name: 'hadoop-aws'
    url: 'https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-{{ version.hadoop }}.jar'
  - name: 'spark-hadoop-cloud'
    url: 'https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.13/3.3.2/spark-hadoop-cloud_2.13-{{ version.spark }}.jar'
  - name: 'commons-pool2'
    url: 'https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar'    
  - name: 'aws-java-sdk-bundle'
    url: 'https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.433/aws-java-sdk-bundle-1.12.433.jar'    
    